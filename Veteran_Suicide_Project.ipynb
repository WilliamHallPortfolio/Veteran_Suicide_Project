{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsYDnsXI7+zjswmnhxdHrF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamHallPortfolio/Veteran_Suicide_Project/blob/main/Veteran_Suicide_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYWtxzWGQUmQ",
        "outputId": "27b66732-a05f-4491-8faf-21d8e0fe315c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/EDUCATION/DATA SCIENCE/ZZZZ ALL PROJECTS/01 PYTHON/02 Veteran Suicide/Data/vaSuicidePreventionInnovation_and_Data_Dictionary/vaSuicidePreventionInnovation.sas7bdat'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sas7bdat pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ5yqEPNSBhS",
        "outputId": "cb2e8028-bda6-4d2d-dc96-bf1b50db6c58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sas7bdat in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from sas7bdat) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EqXI70lOSYtZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example function to process a chunk of data\n",
        "def process(chunk):\n",
        "    # Perform your processing here, e.g., cleaning or transformations\n",
        "    print(f\"Processing a chunk with {len(chunk)} rows\")\n",
        "    # Example: just return the chunk size for demonstration\n",
        "    return len(chunk)\n",
        "\n",
        "# Read and process the SAS file in chunks\n",
        "file_path = '/content/drive/My Drive/EDUCATION/DATA SCIENCE/ZZZZ ALL PROJECTS/01 PYTHON/02 Veteran Suicide/Data/vaSuicidePreventionInnovation_and_Data_Dictionary/vaSuicidePreventionInnovation.sas7bdat'  # Adjust the path as needed\n",
        "\n",
        "# Process in chunks\n",
        "chunk_sizes = []\n",
        "for chunk in pd.read_sas(file_path, chunksize=10000):\n",
        "    chunk_size = process(chunk)  # Call the process function\n",
        "    chunk_sizes.append(chunk_size)\n",
        "\n",
        "print(\"Processing complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tupJskt-Sih8",
        "outputId": "36d076fc-c819-4478-ee2c-90a2e14dd95a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 10000 rows\n",
            "Processing a chunk with 9184 rows\n",
            "Processing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(chunk_sizes, ignore_index=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "SSQ8ua9sTqQU",
        "outputId": "7492b2c8-29fd-4932-dc13-b5b01bd9c4f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot concatenate object of type '<class 'int'>'; only Series and DataFrame objs are valid",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-30a0f04c8973>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ndims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sample_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m_get_ndims\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 )\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'int'>'; only Series and DataFrame objs are valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1aOVes4zWQh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}